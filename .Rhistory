q()
knitr::opts_chunk$set(echo = TRUE)
#install.packages("fitdistrplus")
library(dplyr)
library(Hmisc)
library(fitdistrplus)
library(logspline)
library(Sim.DiffProc)
library(gap)
library(DescTools)
library(gamlss)
library(gamlss.dist)
library(gamlss.add)
library(hash)
library(reshape)
library(plotly)
library(glmnet)
library(caret)
library(randomForest)
library(sail)
library(splitTools)
library(groupdata2)
X_train <- read.csv(file = './data/X_train.csv')
y_train <- read.csv(file = './data/y_train.csv')
X_test <- read.csv(file = './data/X_test.csv')
dim(X_train)
dim(X_test)
dim(y_train)
length(select_if(X_train,is.numeric))
length(select_if(X_test,is.numeric))
str(y_train)
sum(is.na(X_train))
sum(is.na(X_test))
sum(is.na(y_train))
summary(y_train)
y_train.vector <- unlist(as.vector(y_train))
hist(y_train.vector,breaks = 45)
descdist(y_train.vector, discrete = FALSE, boot=500)
fit.unif <- fitdist(y_train.vector, "unif")
fit.unif
plot(fit.unif)
fit.norm <- fitdist(y_train.vector, "norm")
plot(fit.norm)
fit.norm$aic
fit.unif$aic
set.seed(5)
sample.unif.distribution <- runif(3794, min=0, max=2.864712	) # tworzymy wektor z rozkładu jednostajnego o wyestymowanych wcześniej min i max
ks.test(y_train.vector, sample.unif.distribution)
fit.norm <- fitdist(y_train.vector, "norm")
fit.exp <- fitdist(y_train.vector, "exp")
fit.unif <- fitdist(y_train.vector, "unif")
cdfcomp(list(fit.norm, fit.exp, fit.unif),
legendtext = c("norm", "exp", "unif"))
fit <- fitDist(y_train.vector, k = 2, type = "realplus", trace = FALSE, try.gamlss = TRUE)
summary(fit)
sample.exp.distribution <- rexp(3794, rate = 0.115873) # tworzymy wektor z rozkładu wykładniczego z wyestymwoanym parametrem
ks.test(y_train.vector, sample.exp.distribution)
get_most_correlated <- function(x,y,n) {
# funkcja zwracająca nazwy n najbardziej skorelowanych zmiennych ze zmienną objaśnianą
# input: x - DataFrame odpowiadający zmiennym objaśniającym, y - dataframe odpowiadający zmiennej objaśnianej,
#        n - zmienna typu integer ile skorelowanych zmiennych chcemy zwrócić
# output: lista nazw zmiennych
h <- list() # w tej liście przechowamy informację o korelacjach
for (i in 1:ncol(x))
{
h[[colnames(x[i])]] <- abs(cor.test(unlist(as.vector(x[i])), unlist(as.vector(y)), method="pearson")$estimate)
# dla każdej zmiennej wprowadzamy wartość jej korelacji ze zmienną objaśnianą do listy typu key-value
}
h <- h[order(unlist(h), decreasing=TRUE)] # sortujemy listę malejąco według wartości
return(names(h[1:n])) # zwracamy n pierwszych wartości
}
most_correlated <- get_most_correlated(X_train,y_train,250) # zgarniamy 250 najardziej skorelowanych wartości
corr <- cor(X_train[,c(most_correlated)]) # zgarniamy korelacje między sobą pomiędzy uzyskanymi zmiennymi
corr <- round(corr, 2) # wyniki zaokrąglamy
plot_ly(z = corr, x = most_correlated, y = most_correlated, type = "heatmap")
grid <- expand.grid(alpha = c(0, 0.2, 0.4, 0.6, 0.8, 1),
lambda = c(0.001, 0.01, 0.1, 1, 10))
grid
set.seed(1)
elnet.models <- list() # tu dodamy kolejne modele elastic net
X_train_scaled = scale(X_train)
X_test_scaled = scale(X_test, center=attr(X_train_scaled, "scaled:center"),
scale=attr(X_train_scaled, "scaled:scale"))
X_train_scaled <- data.frame(X_train_scaled)
X_test_scaled <- data.frame(X_test_scaled)
folds <- split(X_train_scaled, sample(1:5, nrow(X_train_scaled), replace=T)) # dzielimy dataset na 5 podzbiorów
ind <- list() # indeksy kolejnych podzbiorów
for (j in 1:5){
ind <- append(ind,list(rownames(data.frame(folds[j])))) # zgarniamy indeksy kolejnych podzbiorów
}
for (j in 1:5){
print('.')
elnet <- train(x=X_train_scaled[unlist(ind[j]),], y= y_train[as.numeric(unlist(ind[j])),], method = "glmnet",  tuneGrid=as.data.frame(grid))
print('..')
elnet.models <- list(elnet.models, elnet)
}
elnet.models
flds <- createFolds(y=unlist(y_train), k = 5, list = TRUE, returnTrain = FALSE)
train.control <- trainControl(
index=flds
, verboseIter = T
, returnData = T
, savePredictions = T
)
set.seed(1)
X_train_scaled = scale(X_train)
X_test_scaled = scale(X_test, center=attr(X_train_scaled, "scaled:center"),
scale=attr(X_train_scaled, "scaled:scale"))
X_train_scaled <- data.frame(X_train_scaled)
X_test_scaled <- data.frame(X_test_scaled)
#cv_5 <- trainControl(method = "cv", number = 5)
y_train <- as.numeric(unlist(y_train))
elnet <- train(x=X_train_scaled, y=y_train, method = "glmnet",trControl = train.control, tuneGrid=as.data.frame(grid))
elnet
elnet$resample
mean(elnet$resample[,'RMSE'])
y_train.pred <- predict(elnet, X_train_scaled) # dokonujemy predykcji na zbiorze treningowym
train.error <- sqrt(mean(( unlist(as.vector(y_train)) - y_train.pred)^2))
train.error
grid <- expand.grid(
mtry = c(200,250,300,350,400),
ntree = c(2, 3, 4),
nodesize= c(5, 10, 15, 20))
grid$mtry[3]
set.seed(1)
X_train.ext <- X_train
X_train.ext$y <- y_train
# typeof(X_train.ext)
#
# folds <- split(X_train.ext, sample(1:5, nrow(X_train.ext), replace=T)) # dzielimy dataset na 5 podzbiorów
#
# ind <- list() # indeksy kolejnych podzbiorów
#
# for (j in 1:5){
#
#   ind <- append(ind,list(rownames(data.frame(folds[j])))) # zgarniamy indeksy kolejnych podzbiorów
#
# }
# Bierzemy te same indeksy co przy elasticnet
mse.list <- list() # tu będziemy wrzucać wyniki dla zbiory walidacyjnego dla danego zestawu hiperparametrów
for (i in 1:nrow(grid)){
mse.oneforest <- list() # lista mse dla zbiorów z cv dla bieżącego drzewa
for (j in 1:5){
my.randomForest <- randomForest(x=X_train.ext[unlist(flds[j]),c(1:9000)], y= X_train.ext[unlist(flds[j]),c(9001)],
mtry=grid$mtry[i], ntree=grid$ntree[i], nodesize=grid$nodesize[i])
mse.oneforest <- append(mse.oneforest, tail(my.randomForest$mse, n=1)) # https://stat.ethz.ch/pipermail/r-help/2004-April/049943.html ,                                                          ostatni zwracany mse to mse całego lasu
}
mse.list <- append(mse.list, mean(unlist((mse.oneforest)))) # dodajemy średnią z listy dla bieżącego drzewa
}
mse.list
mse.min.ind <- which.min(mse.list)
grid[mse.min.ind,]
final.forest.rmse.cv <- list()
for (j in 1:5){
final.randomForest <- randomForest(x=X_train.ext[unlist(flds[j]),c(1:9000)], y= X_train.ext[unlist(flds[j]),c(9001)],
mtry=grid$mtry[mse.min.ind], ntree=grid$ntree[mse.min.ind], nodesize=grid$nodesize[mse.min.ind])
final.forest.rmse.cv <- append(final.forest.rmse.cv, sqrt(tail(final.randomForest$mse, n=1))) # https://stat.ethz.ch/pipermail/r-help/2004-April/049943.html ,                                                          ostatni zwracany mse to mse całego lasu, sqrt bo chcemy rmse
}
final.forest.rmse.cv
unlist(final.forest.rmse.cv)
ElasticNet <- unlist(elnet$resample['RMSE'])
RandomForest <- unlist(final.forest.rmse.cv)
summary.df<- data.frame(ElasticNet, RandomForest)
rownames(summary.df) <- c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")
summary.df
elnet.train.error <- train.error
final.randomForest <- randomForest(x=X_train.ext[,c(1:9000)], y= X_train.ext[,c(9001)],
mtry=grid$mtry[mse.min.ind], ntree=grid$ntree[mse.min.ind], nodesize=grid$nodesize[mse.min.ind])
random_forest.train.error <- sqrt(tail(final.randomForest$mse, n=1))
elnet.train.error
random_forest.train.error
reference.rmse.cv <- list()
for (j in 1:5){
avg.y_train <- mean(X_train.ext[unlist(flds[j]),c(9001)])
reference.rmse.cv <- append(reference.rmse.cv, sqrt(mean((X_train.ext[unlist(flds[j]),c(9001)] - avg.y_train)^2)))
}
Reference <- unlist(reference.rmse.cv)
summary.df$ReferenceModel <- Reference
summary.df
set.seed(1)
cv_5 = trainControl(method = "cv", number = 5)
final.elnet <- train(
x = X_train, y = y_train,
method = "glmnet",
trControl = cv_5
)
final.elnet
print('.')
tunegrid <- expand.grid(.mtry=1000)
final.rf <- train(
x = X_train, y = y_train,
method = "rf",
trControl = cv_5,
tuneGrid=tunegrid
)
final.rf
final.predictions <- predict(final.elnet, newdata = X_test)
Id <- c(0:669)
final.df <- data.frame(Id,final.predictions)
colnames(final.df) <- c('Id','Expected')
write.csv(final.df, file="predictions.csv", row.names = FALSE)
c(500, 3000)
print(final.rf)
print(final.randomForest)
print(final.randomForest)
grid <- expand.grid(
mtry = c(1000,5000),
ntree = c(500,750))
grid <- expand.grid(
mtry = c(1000,5000),
ntree = c(500,750))
set.seed(1)
mse.list <- list() # tu będziemy wrzucać wyniki dla zbiory walidacyjnego dla danego zestawu hiperparametrów
for (i in 1:nrow(grid)){
mse.oneforest <- list() # lista mse dla zbiorów z cv dla bieżącego drzewa
for (j in 1:5){
my.randomForest <- randomForest(x=X_train.ext[unlist(flds[j]),c(1:9000)], y= X_train.ext[unlist(flds[j]),c(9001)],
mtry=grid$mtry[i], ntree=grid$ntree[i], nodesize=grid$nodesize[i])
mse.oneforest <- append(mse.oneforest, tail(my.randomForest$mse, n=1)) # https://stat.ethz.ch/pipermail/r-help/2004-April/049943.html ,                                                          ostatni zwracany mse to mse całego lasu
}
mse.list <- append(mse.list, mean(unlist((mse.oneforest)))) # dodajemy średnią z listy dla bieżącego drzewa
}
mse.min.ind <- which.min(mse.list)
grid[mse.min.ind,]
# final.predictions <- predict(final.elnet, newdata = X_test)
Id <- c(0:669)
final.df <- data.frame(Id,final.predictions)
colnames(final.df) <- c('Id','Expected')
write.csv(final.df, file="predictions.csv", row.names = FALSE)
mse.min.ind <- which.min(mse.list)
print(grid[mse.min.ind,])
print(grid)
mse.min.ind <- which.min(mse.list)
grid[mse.min.ind,]
final.forest.rmse.cv <- list()
for (j in 1:5){
final.randomForest <- randomForest(x=X_train.ext[unlist(flds[j]),c(1:9000)], y= X_train.ext[unlist(flds[j]),c(9001)],
mtry=grid$mtry[mse.min.ind], ntree=grid$ntree[mse.min.ind])
final.forest.rmse.cv <- append(final.forest.rmse.cv, sqrt(tail(final.randomForest$mse, n=1))) # https://stat.ethz.ch/pipermail/r-help/2004-April/049943.html ,                                                          ostatni zwracany mse to mse całego lasu, sqrt bo chcemy rmse
}
final.forest.rmse.cv
# final.predictions <- predict(final.elnet, newdata = X_test)
final.predictions <- predict(final.randomForest, newdata = X_test)
Id <- c(0:669)
final.df <- data.frame(Id,final.predictions)
colnames(final.df) <- c('Id','Expected')
write.csv(final.df, file="predictions.csv", row.names = FALSE)
print(final.randomForest)
final.forest.rmse.cv <- list()
for (j in 1:5){
final.randomForest <- randomForest(x=X_train.ext[,c(1:9000)], y= X_train.ext[,c(9001)],
mtry=grid$mtry[mse.min.ind], ntree=grid$ntree[mse.min.ind])
final.forest.rmse.cv <- append(final.forest.rmse.cv, sqrt(tail(final.randomForest$mse, n=1))) # https://stat.ethz.ch/pipermail/r-help/2004-April/049943.html ,                                                          ostatni zwracany mse to mse całego lasu, sqrt bo chcemy rmse
}
